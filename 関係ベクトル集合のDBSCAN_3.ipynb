{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuki\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "c:\\users\\yuki\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "c:\\users\\yuki\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"latest-ja-word2vec-gensim-model/word2vec.gensim.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'AppleGothic'    \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num = pd.get_option(\"display.max_columns\")\n",
    "pd.set_option('display.max_columns', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('sample_except_outlaier_nounvec_list.binaryfile','rb')\n",
    "sample_except_outlaier_nounvec_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11078"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_except_outlaier_nounvec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "relation_vec_list = []\n",
    "\n",
    "for start in range(len(sample_except_outlaier_nounvec_list)):\n",
    "    f_name = \"./relation_vec_fold/\" + str(start) + \".binaryfile\"\n",
    "    \n",
    "    for i in range(start+1, len(sample_except_outlaier_nounvec_list)):\n",
    "            relarion_vec = sample_except_outlaier_nounvec_list[start] - sample_except_outlaier_nounvec_list[i] \n",
    "            relation_vec_list.append(relarion_vec) \n",
    "            \n",
    "    f = open(f_name,'wb')\n",
    "    pickle.dump(relation_vec_list,f)\n",
    "    f.close\n",
    "    relation_vec_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"zip_test\", \"zip\", root_dir=\"./relation_vec_fold/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル一覧\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "sum_list = []\n",
    "\n",
    "files = glob.glob(\"./relation_vec_fold/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_list = random.sample(files, int(len(files)/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功 ./relation_vec_fold\\6311.binaryfile\n",
      "成功 ./relation_vec_fold\\4659.binaryfile\n",
      "成功 ./relation_vec_fold\\2243.binaryfile\n",
      "成功 ./relation_vec_fold\\9027.binaryfile\n",
      "成功 ./relation_vec_fold\\7753.binaryfile\n",
      "成功 ./relation_vec_fold\\9983.binaryfile\n",
      "成功 ./relation_vec_fold\\8746.binaryfile\n",
      "成功 ./relation_vec_fold\\2347.binaryfile\n",
      "成功 ./relation_vec_fold\\9087.binaryfile\n",
      "成功 ./relation_vec_fold\\5748.binaryfile\n",
      "成功 ./relation_vec_fold\\5539.binaryfile\n",
      "50404\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for file in random_list:\n",
    "    try:\n",
    "        f = open(file,'rb')\n",
    "        \n",
    "        relation_vec_list = pickle.load(f)\n",
    "    \n",
    "        sum_list.extend(relation_vec_list)\n",
    "        print(\"成功\", file)\n",
    "        \n",
    "    except:\n",
    "        print(\"失敗\", file)\n",
    "print(len(sum_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./sample_relation_vec_list\", \"wb\")\n",
    "pickle.dump(sum_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"./sample_relation_vec_list\", \"rb\")\n",
    "sum_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_vec_list数 :  50404\n",
      "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
      "    metric_params=None, min_samples=2, n_jobs=None, p=None)\n",
      "y_dbscan [-1  0 -1 ... 47 -1 -1]\n",
      "eps: 0.5 ,minPts: 2\n",
      "outlier数 5036\n",
      "クラスタ数 99\n",
      "クラスタ1に含まれる点の数 2\n",
      "クラスタ2に含まれる点の数 161\n",
      "クラスタ3に含まれる点の数 11\n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "#eps:0.5, min:2\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "eps_minPts = {}\n",
    "minPts_data = {}\n",
    "\n",
    "print(\"noun_vec_list数 : \", len(sum_list))\n",
    "\n",
    "\n",
    "eps = 5/10\n",
    "minPts = 2\n",
    "        \n",
    "dbscan = DBSCAN(eps=eps,min_samples=minPts).fit(relation_vec_list)\n",
    "        \n",
    "print(dbscan)\n",
    "        \n",
    "y_dbscan = dbscan.labels_\n",
    "        \n",
    "print(\"y_dbscan\", y_dbscan)\n",
    "        \n",
    "print(\"eps:\",eps,\",minPts:\", minPts)\n",
    "        \n",
    "        # outlier数\n",
    "print(\"outlier数\", len(np.where(y_dbscan ==-1)[0]))\n",
    "        \n",
    "        # クラスタ数\n",
    "print(\"クラスタ数\", np.max(y_dbscan) +2)\n",
    "        \n",
    "        # クラスタ1に含まれる点の数\n",
    "print(\"クラスタ1に含まれる点の数\", len(np.where(y_dbscan ==0)[0]))\n",
    "        \n",
    "        # クラスタ2に含まれる点の数\n",
    "print(\"クラスタ2に含まれる点の数\", len(np.where(y_dbscan ==1)[0]))\n",
    "        \n",
    "        # クラスタ3に含まれる点の数\n",
    "print(\"クラスタ3に含まれる点の数\", len(np.where(y_dbscan ==2)[0]))\n",
    "        \n",
    "print(\"                            \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_vec_list数 :  50404\n",
      "y_dbscan [ -1  -1  -1 ... 225  -1  -1]\n",
      "eps: 0.4 ,minPts: 2\n",
      "outlier数 49777\n",
      "クラスタ数 240\n",
      "クラスタ1に含まれる点の数 2\n",
      "クラスタ2に含まれる点の数 5\n",
      "クラスタ3に含まれる点の数 3\n",
      "                            \n",
      "y_dbscan [-1 -1 -1 ... -1 -1 -1]\n",
      "eps: 0.4 ,minPts: 3\n",
      "outlier数 50107\n",
      "クラスタ数 75\n",
      "クラスタ1に含まれる点の数 3\n",
      "クラスタ2に含まれる点の数 5\n",
      "クラスタ3に含まれる点の数 3\n",
      "                            \n",
      "y_dbscan [-1 -1 -1 ... -1 -1 -1]\n",
      "eps: 0.4 ,minPts: 4\n",
      "outlier数 50323\n",
      "クラスタ数 14\n",
      "クラスタ1に含まれる点の数 4\n",
      "クラスタ2に含まれる点の数 5\n",
      "クラスタ3に含まれる点の数 5\n",
      "                            \n",
      "y_dbscan [ -1  -1  -1 ... 843  -1  -1]\n",
      "eps: 0.5 ,minPts: 2\n",
      "outlier数 45739\n",
      "クラスタ数 895\n",
      "クラスタ1に含まれる点の数 3\n",
      "クラスタ2に含まれる点の数 2\n",
      "クラスタ3に含まれる点の数 48\n",
      "                            \n",
      "y_dbscan [-1 -1 -1 ... -1 -1 -1]\n",
      "eps: 0.5 ,minPts: 3\n",
      "outlier数 46957\n",
      "クラスタ数 286\n",
      "クラスタ1に含まれる点の数 3\n",
      "クラスタ2に含まれる点の数 48\n",
      "クラスタ3に含まれる点の数 117\n",
      "                            \n",
      "y_dbscan [-1 -1 -1 ... -1 -1 -1]\n",
      "eps: 0.5 ,minPts: 4\n",
      "outlier数 47526\n",
      "クラスタ数 176\n",
      "クラスタ1に含まれる点の数 39\n",
      "クラスタ2に含まれる点の数 105\n",
      "クラスタ3に含まれる点の数 8\n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "eps_minPts = {}\n",
    "minPts_data = {}\n",
    "\n",
    "print(\"noun_vec_list数 : \", len(sum_list))\n",
    "\n",
    "for eps in range(4, 6):\n",
    "    eps = eps/10\n",
    "    \n",
    "    for minPts in range(2, 5):\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps,min_samples=minPts).fit(sum_list)\n",
    "        \n",
    "        #print(dbscan)\n",
    "        \n",
    "        y_dbscan = dbscan.labels_\n",
    "        \n",
    "        print(\"y_dbscan\", y_dbscan)\n",
    "        \n",
    "        print(\"eps:\",eps,\",minPts:\", minPts)\n",
    "        \n",
    "        # outlier数\n",
    "        print(\"outlier数\", len(np.where(y_dbscan ==-1)[0]))\n",
    "        \n",
    "        # クラスタ数\n",
    "        print(\"クラスタ数\", np.max(y_dbscan) +2)\n",
    "        \n",
    "        # クラスタ1に含まれる点の数\n",
    "        print(\"クラスタ1に含まれる点の数\", len(np.where(y_dbscan ==0)[0]))\n",
    "        \n",
    "        # クラスタ2に含まれる点の数\n",
    "        print(\"クラスタ2に含まれる点の数\", len(np.where(y_dbscan ==1)[0]))\n",
    "        \n",
    "        # クラスタ3に含まれる点の数\n",
    "        print(\"クラスタ3に含まれる点の数\", len(np.where(y_dbscan ==2)[0]))\n",
    "        \n",
    "        print(\"                            \")\n",
    "        \n",
    "        minPts_data[minPts] = y_dbscan\n",
    "        eps_minPts[eps] = minPts_data\n",
    "        \n",
    "    f_name = \"./sample_relation_vec_DBSCAN_fold/\" + str(eps)\n",
    "    f = open(f_name, \"wb\")\n",
    "    pickle.dump(eps_minPts, f)\n",
    "    f.close\n",
    "    \n",
    "    eps_minPts = {}\n",
    "    minPts_data = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
